Automatically generated by Mendeley Desktop 1.14
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Kim2009,
annote = {Her st\aa r vistnok at out-of-bootstrap er nice nok},
author = {Kim, JH},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Kim - 2009 - Estimating classification error rate Repeated cross-validation, repeated hold-out and bootstrap.pdf:pdf},
journal = {Computational Statistics \& Data Analysis},
title = {{Estimating classification error rate: Repeated cross-validation, repeated hold-out and bootstrap}},
url = {http://www.sciencedirect.com/science/article/pii/S0167947309001601},
year = {2009}
}
@article{Guyon2002,
author = {Guyon, Isabelle},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Guyon - 2002 - Gene Selection for Cancer Classification.pdf:pdf},
keywords = {array,cancer classification,diagnosis,diagnostic tests,dna micro-,drug discovery,feature selection,gene selection,genomics,proteomics,recursive feature elimination,rna expression,support vector machines},
pages = {389--422},
title = {{Gene Selection for Cancer Classification}},
year = {2002}
}
@article{Osuna,
author = {Osuna, Edgar and Platt, John},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Osuna, Platt - Unknown - Support vector machines.pdf:pdf},
title = {{Support vector machines}}
}
@article{fisher_original,
author = {Fisher, RA},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Fisher - 1936 - The use of multiple measurements in taxonomic problems.pdf:pdf},
journal = {Annals of eugenics},
title = {{The use of multiple measurements in taxonomic problems}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/full},
year = {1936}
}
@article{Feiler2015,
author = {Feiler, D. C. and Kleinbaum, a. M.},
doi = {10.1177/0956797615569580},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Feiler, Kleinbaum - 2015 - Popularity, Similarity, and the Network Extraversion Bias.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {14,15,16,6,a fundamental notion of,extraversion,one,personality,received 7,revision accepted 1,s,social judgment,social networks,social psychology is that},
title = {{Popularity, Similarity, and the Network Extraversion Bias}},
url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797615569580},
year = {2015}
}
@article{Pietersma2011,
annote = {Noisy data is bad for SVMs},
author = {Pietersma, AD},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Pietersma - 2011 - Kernel Learning in Support Vector Machines using Dual-Objective Optimization.pdf:pdf},
journal = {Proceedings of the 23rd \ldots},
title = {{Kernel Learning in Support Vector Machines using Dual-Objective Optimization}},
url = {http://www.researchgate.net/profile/Marco\_Wiering/publication/264886577\_Kernel\_Learning\_in\_Support\_Vector\_Machines\_using\_Dual-Objective\_Optimization/links/540868da0cf2bba34c283a50.pdf},
year = {2011}
}
@article{Salton1975,
abstract = {In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.},
author = {Salton, G. and Wong, a. and Yang, C. S.},
doi = {10.1145/361219.361220},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Salton, Wong, Yang - 1975 - A vector space model for automatic indexing.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {and phrases,automatic indexing,automatic information,content analysis,document,retrieval},
number = {11},
pages = {613--620},
pmid = {15142973},
title = {{A vector space model for automatic indexing}},
volume = {18},
year = {1975}
}
@article{ESA,
author = {Gabrilovich, E and Markovitch, S},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Gabrilovich, Markovitch - 2007 - Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.pdf:pdf},
journal = {IJCAI},
title = {{Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.}},
url = {http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-259.pdf},
year = {2007}
}
@article{Walker2009,
abstract = {Areas beyond the classical receptive field (CRF) can modulate responses of the majority of cells in the primary visual cortex of the cat (). Although general characteristics of this phenomenon have been reported previously, little is known about the detailed spatial organization of the surrounds. Previous work suggests that the surrounds may be uniform regions that encircle the CRF or may be limited to the "ends" of the CRF. We have examined the spatial organization of surrounds of single-cell receptive fields in the primary visual cortex of anesthetized, paralyzed cats. The CRF was stimulated with an optimal drifting grating, whereas the surround was probed with a second small grating patch placed at discrete locations around the CRF. For most cells that exhibit suppression, the surrounds are spatially asymmetric, such that the suppression originates from a localized region. We find a variety of suppressive zone locations, but there is a slight bias for suppression to occur at the end zones of the CRF. The spatial pattern of suppression is independent of the parameters of the suppressive stimulus used, although the effect is clearest with iso-oriented surround stimuli. A subset of cells exhibit axially symmetric or uniform surround fields. These results demonstrate that the surrounds are more specific than previously realized, and this specialization has implications for the processing of visual information in the primary visual cortex. One possibility is that these localized surrounds may provide a substrate for figure-ground segmentation of visual scenes.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Walker, G a and Ohzawa, I and Freeman, R D},
doi = {10.1109/LPT.2009.2020494},
eprint = {0521865719 9780521865715},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Walker, Ohzawa, Freeman - 2009 - IEEE Photonics Technology Letters information for authors.pdf:pdf},
isbn = {0521865719},
issn = {1041-1135},
journal = {IEEE Photonics Technology Letters},
keywords = {Animals,Cats,Electrophysiology,Geniculate Bodies,Geniculate Bodies: physiology,Neurons,Neurons: physiology,Ocular,Ocular: physiology,Photic Stimulation,Photic Stimulation: methods,Time Factors,Vision,Vision Tests,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology},
number = {8},
pages = {C3--C3},
pmid = {10575050},
title = {{IEEE Photonics Technology Letters information for authors}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4812360$\backslash$nhttp://www.ncbi.nlm.nih.gov/pubmed/10575050},
volume = {21},
year = {2009}
}
@article{Salton1988,
abstract = {The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared.},
archivePrefix = {arXiv},
arxivId = {115},
author = {Salton, Gerard and Buckley, Christopher},
doi = {10.1016/0306-4573(88)90021-0},
eprint = {115},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Salton, Buckley - 1988 - Term-weighting approaches in automatic text retrieval.pdf:pdf},
isbn = {1558604545},
issn = {03064573},
journal = {Information Processing \& Management},
number = {5},
pages = {513--523},
title = {{Term-weighting approaches in automatic text retrieval}},
volume = {24},
year = {1988}
}
@article{Manning2009,
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
archivePrefix = {arXiv},
arxivId = {0521865719 9780521865715},
author = {Manning, Christopher D. and Raghavan, Prabhakar and Schutze, Hinrich},
doi = {10.1109/LPT.2009.2020494},
eprint = {0521865719 9780521865715},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Manning, Raghavan, Schutze - 2009 - An Introduction to Information Retrieval.pdf:pdf},
isbn = {0521865719},
issn = {13864564},
journal = {Online},
keywords = {keyword},
number = {c},
pages = {569},
pmid = {10575050},
title = {{An Introduction to Information Retrieval}},
url = {http://dspace.cusat.ac.in/dspace/handle/123456789/2538},
year = {2009}
}
@article{Nakov2000,
author = {Nakov, Preslav},
doi = {10.1145/365143.365382},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Nakov - 2000 - Latent semantic analysis of textual data.pdf:pdf},
isbn = {9549641171},
journal = {Proceedings of the conference on Computer systems and technologies - CompSysTech '00},
pages = {5031--5035},
title = {{Latent semantic analysis of textual data}},
url = {http://portal.acm.org/citation.cfm?doid=365143.365382},
year = {2000}
}
@article{Blum1999,
author = {Blum, Avrim and Kalai, Adam and Langford, John},
doi = {10.1145/307400.307439},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Blum, Kalai, Langford - 1999 - Beating the Hold-out Bounds for \{K\}-fold and Progressive Cross-Validation.pdf:pdf},
isbn = {1581131674},
journal = {Proceedings of the 12th Annual Conference on Computational Learning Theory},
number = {c},
pages = {203--208},
title = {{Beating the Hold-out: Bounds for \{K\}-fold and Progressive Cross-Validation}},
url = {http://portal.acm.org/citation.cfm?doid=307400.307439},
year = {1999}
}
@article{Digman1990a,
author = {Digman, JM},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Digman - 1990 - Personality structure Emergence of the five-factor model.pdf:pdf},
journal = {Annual review of psychology},
title = {{Personality structure: Emergence of the five-factor model}},
url = {http://www.annualreviews.org/doi/pdf/10.1146/annurev.ps.41.020190.002221},
year = {1990}
}
@article{burges1998,
abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Burges, Cjc Christopher J C},
doi = {10.1023/A:1009715923555},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Burges - 1998 - A Tutorial on Support Vector Machines for Pattern Recognition.pdf:pdf},
isbn = {0818672404},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
number = {2},
pages = {121--167},
pmid = {5207842081938259593},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
url = {http://www.springerlink.com/index/Q87856173126771Q.pdf$\backslash$nhttp://link.springer.com/article/10.1023/A:1009715923555},
volume = {2},
year = {1998}
}
@article{wolfe1961duality,
author = {Wolfe, Philip},
journal = {Quarterly of applied mathematics},
number = {3},
pages = {239--244},
title = {{A duality theorem for nonlinear programming}},
volume = {19},
year = {1961}
}
@article{Sekara2014,
abstract = {Understanding how people interact and socialize is important in many contexts from disease control to urban planning. Datasets that capture this specific aspect of human life have increased in size and availability over the last few years. We have yet to understand, however, to what extent such electronic datasets may serve as a valid proxy for real life social interactions. For an observational dataset, gathered using mobile phones, we analyze the problem of identifying transient and non-important links, as well as how to highlight important social interactions. Applying the Bluetooth signal strength parameter to distinguish between observations, we demonstrate that weak links, compared to strong links, have a lower probability of being observed at later times, while such links—on average—also have lower link-weights and probability of sharing an online friendship. Further, the role of link-strength is investigated in relation to social network properties. [ABSTRACT FROM AUTHOR]},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.5836v3},
author = {Sekara, Vedran and Lehmann, Sune},
doi = {10.1371/journal.pone.0100915},
eprint = {arXiv:1401.5836v3},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Sekara, Lehmann - 2014 - The strength of friendship ties in proximity sensor data.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--14},
pmid = {24999984},
title = {{The strength of friendship ties in proximity sensor data}},
volume = {9},
year = {2014}
}
@article{MJW,
author = {Williams, MJ},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Williams - 2012 - Measuring individual regularity in human visiting patterns.pdf:pdf},
journal = {Privacy, Security, Risk and \ldots},
title = {{Measuring individual regularity in human visiting patterns}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6406276},
year = {2012}
}
@article{Landauer1998,
abstract = {Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text (Landauer and Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.},
author = {Landauer, Thomas K and Foltz, Peter W. and Laham, Darrell},
doi = {10.1080/01638539809545028},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Landauer, Foltz, Laham - 1998 - An introduction to latent semantic analysis.pdf:pdf},
isbn = {0163853X},
issn = {0163-853X},
journal = {Discourse Processes},
number = {2-3},
pages = {259--284},
pmid = {14532333},
title = {{An introduction to latent semantic analysis}},
volume = {25},
year = {1998}
}
@article{Blum1997,
abstract = {In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.},
annote = {Irrelevante features fucker NN-modeller helt op (\#n\o dveidge tr\ae ningspunkter stiger eksponentielt med antal irrelevante features.)},
author = {Blum, Avrim L. and Langley, Pat},
doi = {10.1016/S0004-3702(97)00063-5},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Machine learning,Relevant examples,Relevant features},
month = dec,
number = {1-2},
pages = {245--271},
title = {{Selection of relevant features and examples in machine learning}},
url = {http://www.sciencedirect.com/science/article/pii/S0004370297000635},
volume = {97},
year = {1997}
}
@article{hearst1998,
abstract = {My first exposure to Support Vector Machines came this spring when heard Sue Dumais present impressive results on text categorization using this analysis technique. This issue's collection of essays should help familiarize our readers with this interesting new racehorse in the Machine Learning stable. Bernhard Scholkopf, in an introductory overview, points out that a particular advantage of SVMs over other learning algorithms is that it can be analyzed theoretically using concepts from computational learning theory, and at the same time can achieve good performance when applied to real problems. Examples of these real-world applications are provided by Sue Dumais, who describes the aforementioned text-categorization problem, yielding the best results to date on the Reuters collection, and Edgar Osuna, who presents strong results on application to face detection. Our fourth author, John Platt, gives us a practical guide and a new technique for implementing the algorithm efficiently},
author = {Hearst, Marti a. and Dumais, Susan T. and Osuna, Edgar and Platt, John and Sch\"{o}lkopf, Bernhard},
doi = {10.1109/5254.708428},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Hearst et al. - 1998 - Support vector machines.pdf:pdf},
isbn = {9780471273981},
issn = {1094-7167},
journal = {IEEE Intelligent Systems and their Applications},
number = {4},
pages = {18--28},
pmid = {21244189},
title = {{Support vector machines}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=708428},
volume = {13},
year = {1998}
}
@book{Boyd2004,
abstract = {Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.},
annote = {Den som forklarer Slack variables},
author = {Boyd, Stephen and Vandenberghe, Lieven},
isbn = {1107394007},
pages = {716},
publisher = {Cambridge University Press},
title = {{Convex Optimization}},
url = {https://books.google.com/books?hl=en\&lr=\&id=IUZdAAAAQBAJ\&pgis=1},
year = {2004}
}
@book{Smola1998,
author = {Smola, AJ and Sch\"{o}lkopf, B},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Smola, Sch\"{o}lkopf - 1998 - Learning with kernels.pdf:pdf},
title = {{Learning with kernels}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.4783\&rep=rep1\&type=pdf},
year = {1998}
}
@article{gps_precision,
author = {Zandbergen, PA and Barbeau, SJ},
journal = {Journal of Navigation},
title = {{Positional accuracy of assisted gps data from high-sensitivity gps-enabled mobile phones}},
url = {http://journals.cambridge.org/abstract\_S0373463311000051},
year = {2011}
}
@article{Digman1990,
author = {Digman, JM John M},
file = {:C$\backslash$:/Dropbox/speciale/litterature//Digman - 1990 - Personality structure Emergence of the five-factor model.pdf:pdf;:C$\backslash$:/Dropbox/speciale/litterature/Digman - 1990 - Personality structure Emergence of the five-factor model.pdf:pdf},
journal = {Annual review of psychology},
pages = {417--440},
title = {{Personality structure: Emergence of the five-factor model}},
url = {http://www.annualreviews.org/doi/pdf/10.1146/annurev.ps.41.020190.002221},
volume = {41},
year = {1990}
}
@article{Burges1998a,
author = {Burges, Christopher J C},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Burges - 1998 - . ' ' 1-43 () A Tutorial on Support Vector Machines for Pattern Recognition.pdf:pdf},
keywords = {pattern recognition,statistical learning theory,support vector machines,vc dimension},
pages = {121--167},
title = {{. ' ' 1-43 () A Tutorial on Support Vector Machines for Pattern Recognition}},
volume = {43},
year = {1998}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre- lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna- tional conference, ∗∗∗, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression. Keywords:},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023\%2FA\%3A1010933404324},
author = {Breiman, L},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023\%2FA\%3A1010933404324},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Breiman - 2001 - Random forests.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
pmid = {21816105},
primaryClass = {http:},
title = {{Random forests}},
url = {http://link.springer.com/article/10.1023/A:1010933404324},
year = {2001}
}
@article{Smola2004,
annote = {Tutorial som introducerer SVR.},
author = {Smola, AJ and Sch\"{o}lkopf, B},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Smola, Sch\"{o}lkopf - 2004 - A tutorial on support vector regression.pdf:pdf},
journal = {Statistics and computing},
title = {{A tutorial on support vector regression}},
url = {http://link.springer.com/article/10.1023/B:STCO.0000035301.49549.88},
year = {2004}
}
@article{Bengio2004,
abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances.  In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates.  This paper studies the estimation of uncertainty around the K-fold cross-validation estimator.  The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation.  An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as confirmed by numerical experiments.},
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {:C$\backslash$:/Dropbox/speciale/litterature/Bengio, Grandvalet - 2004 - No Unbiased Estimator of the Variance of K-Fold Cross-Validation.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
keywords = {Learning/Statistics \& Optimisation,Theory \& Algorithms},
pages = {1089--1105},
title = {{No Unbiased Estimator of the Variance of K-Fold Cross-Validation}},
url = {http://eprints.pascal-network.org/archive/00000828/},
volume = {5},
year = {2004}
}
@article{demontjoye2013,
abstract = {The present study provides the first evidence that personal- ity can be reliably predicted from standard mobile phone logs. Using a set of novel psychology-informed indicators that can be computed from data available to all carriers, we were able to predict users’ personality with a mean accuracy across traits of 42\% better than random, reaching up to 61\% accuracy on a three-class problem. Given the fast growing number of mobile phone subscription and availability of phone logs to researchers, our new personality indicators open the door to exciting av- enues for future research in social sciences. They potentially enable cost- effective, questionnaire-free investigation of personality-related questions at a scale never seen before.},
author = {{De Montjoye}, Yves Alexandre and Quoidbach, Jordi and Robic, Florent and Pentland, Alex},
doi = {10.1007/978-3-642-37210-0\_6},
file = {:C$\backslash$:/Dropbox/speciale/litterature/De Montjoye et al. - 2013 - Predicting personality using novel mobile phone-based metrics.pdf:pdf},
isbn = {9783642372094},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Big Data,Big Five Personality prediction,CDR,Carrier's log,Personality prediction},
pages = {48--55},
title = {{Predicting personality using novel mobile phone-based metrics}},
volume = {7812 LNCS},
year = {2013}
}
